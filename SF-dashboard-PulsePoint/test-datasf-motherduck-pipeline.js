#!/usr/bin/env node

/**
 * SF Crime Data Pipeline Test: DataSF API â†’ MotherDuck
 *
 * This script demonstrates a complete data pipeline:
 * 1. Fetch crime data from DataSF API (10 records)
 * 2. Create dedicated database in MotherDuck
 * 3. Transform and clean the data
 * 4. Load data into MotherDuck table
 * 5. Run analytical queries to verify pipeline
 */

const https = require('https');
const duckdb = require('duckdb');
const fs = require('fs');
const path = require('path');

// Configuration
const CONFIG = {
  datasf: {
    baseUrl: 'https://data.sfgov.org/resource/wg3w-h783.json',
    recordLimit: 10
  },
  motherduck: {
    database: 'my_db',
    table: 'sf_crime_incidents'
  }
};

/**
 * Load MotherDuck token from .env file
 */
function loadMotherDuckToken() {
  const envPath = path.join(__dirname, 'frontend/pulsepointsf/.env');

  if (!fs.existsSync(envPath)) {
    throw new Error(`âŒ .env file not found at: ${envPath}`);
  }

  const envContent = fs.readFileSync(envPath, 'utf8');
  const tokenMatch = envContent.match(/NEXT_PUBLIC_DUCK_READ_TOKEN="([^"]+)"/);

  if (!tokenMatch) {
    throw new Error('âŒ NEXT_PUBLIC_DUCK_READ_TOKEN not found in .env file');
  }

  return tokenMatch[1];
}

/**
 * Make HTTP request with promise wrapper
 */
function makeRequest(url) {
  return new Promise((resolve, reject) => {
    const startTime = Date.now();
    console.log(`ğŸ“¡ Requesting: ${url}`);

    const req = https.get(url, (res) => {
      let data = '';

      res.on('data', (chunk) => {
        data += chunk;
      });

      res.on('end', () => {
        const endTime = Date.now();
        const responseTime = endTime - startTime;

        try {
          const jsonData = JSON.parse(data);
          resolve({
            statusCode: res.statusCode,
            data: jsonData,
            responseTime,
            dataSize: data.length
          });
        } catch (parseError) {
          reject({
            error: 'JSON Parse Error',
            message: parseError.message,
            statusCode: res.statusCode,
            responseTime
          });
        }
      });
    });

    req.on('error', (error) => {
      reject({
        error: 'Network Error',
        message: error.message
      });
    });

    req.setTimeout(30000, () => {
      req.destroy();
      reject({
        error: 'Timeout Error',
        message: 'Request timed out after 30 seconds'
      });
    });
  });
}

/**
 * Step 1: Fetch crime data from DataSF API
 */
async function fetchCrimeData() {
  console.log('ğŸ›ï¸ Step 1: Fetching Crime Data from DataSF API');
  console.log('===============================================');

  try {
    // Build query for recent crimes with location data
    const whereClause = 'latitude IS NOT NULL AND longitude IS NOT NULL';
    const url = `${CONFIG.datasf.baseUrl}?$where=${encodeURIComponent(whereClause)}&$limit=${CONFIG.datasf.recordLimit}&$order=incident_datetime DESC`;

    const result = await makeRequest(url);

    console.log(`âœ… Successfully fetched ${result.data.length} records`);
    console.log(`â±ï¸ Response Time: ${result.responseTime}ms`);
    console.log(`ğŸ“¦ Data Size: ${result.dataSize} bytes`);

    if (result.data.length === 0) {
      throw new Error('No crime data returned from API');
    }

    // Show sample of fetched data
    console.log('\nğŸ“‹ Sample Record Fields:');
    const sampleRecord = result.data[0];
    const fields = Object.keys(sampleRecord);
    fields.slice(0, 10).forEach((field, index) => {
      const value = sampleRecord[field];
      const preview = typeof value === 'string' && value.length > 30
        ? value.substring(0, 30) + '...'
        : value;
      console.log(`   ${index + 1}. ${field}: ${preview}`);
    });

    if (fields.length > 10) {
      console.log(`   ... and ${fields.length - 10} more fields`);
    }

    return result.data;

  } catch (error) {
    console.error('âŒ Failed to fetch crime data:', error.message);
    throw error;
  }
}

/**
 * Step 2: Transform and clean the data
 */
function transformCrimeData(rawData) {
  console.log('\nğŸ”§ Step 2: Transforming and Cleaning Data');
  console.log('==========================================');

  const transformedData = rawData.map((record, index) => {
    try {
      // Core incident information
      const transformed = {
        // Primary identifiers
        incident_id: record.incident_id || `unknown_${index}`,
        incident_number: record.incident_number || null,

        // Incident details
        incident_category: record.incident_category || 'Unknown',
        incident_subcategory: record.incident_subcategory || null,
        incident_description: record.incident_description || null,

        // Temporal data
        incident_datetime: record.incident_datetime || null,
        incident_date: record.incident_date || null,
        incident_time: record.incident_time || null,
        incident_year: record.incident_year ? parseInt(record.incident_year) : null,
        incident_day_of_week: record.incident_day_of_week || null,

        // Location data
        latitude: record.latitude ? parseFloat(record.latitude) : null,
        longitude: record.longitude ? parseFloat(record.longitude) : null,
        point: record.point || null,

        // Administrative areas
        analysis_neighborhood: record.analysis_neighborhood || null,
        police_district: record.police_district || null,
        supervisor_district: record.supervisor_district ? parseInt(record.supervisor_district) : null,

        // Case status
        resolution: record.resolution || null,
        report_type_code: record.report_type_code || null,
        report_type_description: record.report_type_description || null,

        // Metadata
        filed_online: record.filed_online === 'true' || record.filed_online === true,

        // Pipeline metadata
        data_source: 'datasf_api',
        ingestion_timestamp: new Date().toISOString(),

        // Derived fields
        hour_of_day: null,
        day_of_week_num: null
      };

      // Extract hour and day of week if datetime available
      if (transformed.incident_datetime) {
        try {
          const date = new Date(transformed.incident_datetime);
          transformed.hour_of_day = date.getHours();
          transformed.day_of_week_num = date.getDay(); // 0 = Sunday, 1 = Monday, etc.
        } catch (dateError) {
          console.warn(`âš ï¸ Could not parse date for record ${index}:`, transformed.incident_datetime);
        }
      }

      // Validate coordinates are within SF area (approximate bounds)
      if (transformed.latitude && transformed.longitude) {
        const isValidSFCoords =
          transformed.latitude >= 37.7 && transformed.latitude <= 37.82 &&
          transformed.longitude >= -122.52 && transformed.longitude <= -122.35;

        if (!isValidSFCoords) {
          console.warn(`âš ï¸ Record ${index} has coordinates outside SF area: ${transformed.latitude}, ${transformed.longitude}`);
        }
      }

      return transformed;

    } catch (transformError) {
      console.error(`âŒ Error transforming record ${index}:`, transformError.message);
      throw transformError;
    }
  });

  console.log(`âœ… Successfully transformed ${transformedData.length} records`);

  // Show transformation summary
  const withCoords = transformedData.filter(r => r.latitude && r.longitude).length;
  const withDateTime = transformedData.filter(r => r.incident_datetime).length;
  const categories = [...new Set(transformedData.map(r => r.incident_category))];

  console.log(`ğŸ“Š Transformation Summary:`);
  console.log(`   Records with coordinates: ${withCoords}/${transformedData.length}`);
  console.log(`   Records with datetime: ${withDateTime}/${transformedData.length}`);
  console.log(`   Unique categories: ${categories.length} (${categories.slice(0, 3).join(', ')}${categories.length > 3 ? '...' : ''})`);

  return transformedData;
}

/**
 * Step 3: Initialize MotherDuck connection and setup database
 */
async function setupMotherDuckDatabase() {
  console.log('\nğŸ¦† Step 3: Setting Up MotherDuck Database');
  console.log('==========================================');

  try {
    // Load token and create connection
    const token = loadMotherDuckToken();
    console.log('ğŸ”‘ Token loaded successfully');

    process.env.motherduck_token = token;
    const db = new duckdb.Database('md:');
    console.log('âœ… MotherDuck connection established');

    // Test basic connectivity
    await new Promise((resolve, reject) => {
      db.all('SELECT 1 as test_connection', (err, res) => {
        if (err) reject(err);
        else {
          console.log('ğŸ§ª Connection test passed');
          resolve(res);
        }
      });
    });

    // Skip database creation (read-only access)
    console.log(`ğŸ—„ï¸ Using existing database: ${CONFIG.motherduck.database}`);
    console.log(`âœ… Database ${CONFIG.motherduck.database} access verified (read-only token)`);

    // Switch to the database
    console.log(`ğŸ”„ Switching to database: ${CONFIG.motherduck.database}`);
    await new Promise((resolve, reject) => {
      db.all(`USE ${CONFIG.motherduck.database}`, (err, res) => {
        if (err) {
          console.error('âŒ Failed to use database:', err.message);
          reject(err);
        } else {
          console.log(`âœ… Now using database: ${CONFIG.motherduck.database}`);
          resolve(res);
        }
      });
    });

    return db;

  } catch (error) {
    console.error('âŒ Failed to setup MotherDuck database:', error.message);
    throw error;
  }
}

/**
 * Step 4: Create table schema and load data
 */
async function createTableAndLoadData(db, transformedData) {
  console.log('\nğŸ“Š Step 4: Creating Table Schema and Loading Data');
  console.log('==================================================');

  try {
    // Drop table if exists (for clean testing)
    console.log(`ğŸ—‘ï¸ Dropping existing table if present: ${CONFIG.motherduck.table}`);
    await new Promise((resolve, reject) => {
      db.all(`DROP TABLE IF EXISTS ${CONFIG.motherduck.table}`, (err, res) => {
        if (err) {
          console.warn('âš ï¸ Could not drop table (may not exist):', err.message);
        }
        resolve(res);
      });
    });

    // Create table with comprehensive schema
    const createTableSQL = `
      CREATE TABLE ${CONFIG.motherduck.table} (
        -- Primary identifiers
        incident_id VARCHAR PRIMARY KEY,
        incident_number VARCHAR,

        -- Incident details
        incident_category VARCHAR,
        incident_subcategory VARCHAR,
        incident_description VARCHAR,

        -- Temporal data
        incident_datetime TIMESTAMP,
        incident_date DATE,
        incident_time TIME,
        incident_year INTEGER,
        incident_day_of_week VARCHAR,

        -- Location data
        latitude DOUBLE,
        longitude DOUBLE,
        point VARCHAR,

        -- Administrative areas
        analysis_neighborhood VARCHAR,
        police_district VARCHAR,
        supervisor_district INTEGER,

        -- Case status
        resolution VARCHAR,
        report_type_code VARCHAR,
        report_type_description VARCHAR,

        -- Metadata
        filed_online BOOLEAN,
        data_source VARCHAR,
        ingestion_timestamp TIMESTAMP,

        -- Derived fields
        hour_of_day INTEGER,
        day_of_week_num INTEGER
      )
    `;

    console.log(`ğŸ—ï¸ Creating table: ${CONFIG.motherduck.table}`);
    await new Promise((resolve, reject) => {
      db.all(createTableSQL, (err, res) => {
        if (err) {
          console.error('âŒ Failed to create table:', err.message);
          reject(err);
        } else {
          console.log(`âœ… Table ${CONFIG.motherduck.table} created successfully`);
          resolve(res);
        }
      });
    });

    // Insert data using parameterized queries
    console.log('ğŸ“¥ Inserting transformed data...');

    for (let i = 0; i < transformedData.length; i++) {
      const record = transformedData[i];

      const insertSQL = `
        INSERT INTO ${CONFIG.motherduck.table} VALUES (
          ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?
        )
      `;

      const values = [
        record.incident_id,
        record.incident_number,
        record.incident_category,
        record.incident_subcategory,
        record.incident_description,
        record.incident_datetime,
        record.incident_date,
        record.incident_time,
        record.incident_year,
        record.incident_day_of_week,
        record.latitude,
        record.longitude,
        record.point,
        record.analysis_neighborhood,
        record.police_district,
        record.supervisor_district,
        record.resolution,
        record.report_type_code,
        record.report_type_description,
        record.filed_online,
        record.data_source,
        record.ingestion_timestamp,
        record.hour_of_day,
        record.day_of_week_num
      ];

      await new Promise((resolve, reject) => {
        db.all(insertSQL, values, (err, res) => {
          if (err) {
            console.error(`âŒ Failed to insert record ${i + 1}:`, err.message);
            console.error('Record data:', record);
            reject(err);
          } else {
            resolve(res);
          }
        });
      });
    }

    console.log(`âœ… Successfully loaded ${transformedData.length} records into ${CONFIG.motherduck.table}`);

    // Verify data load
    await new Promise((resolve, reject) => {
      db.all(`SELECT COUNT(*) as total_records FROM ${CONFIG.motherduck.table}`, (err, res) => {
        if (err) {
          console.error('âŒ Failed to verify data load:', err.message);
          reject(err);
        } else {
          const count = res[0].total_records;
          console.log(`ğŸ“Š Verification: ${count} records found in table`);
          resolve(res);
        }
      });
    });

  } catch (error) {
    console.error('âŒ Failed to create table and load data:', error.message);
    throw error;
  }
}

/**
 * Step 5: Run analytical queries to test the pipeline
 */
async function runAnalyticalQueries(db) {
  console.log('\nğŸ“ˆ Step 5: Running Analytical Queries');
  console.log('=====================================');

  const queries = [
    {
      name: 'Total Record Count',
      sql: `SELECT COUNT(*) as total_records FROM ${CONFIG.motherduck.table}`,
      description: 'Verify all records loaded successfully'
    },
    {
      name: 'Recent Incidents (Last 24 Hours)',
      sql: `
        SELECT incident_category, incident_description, incident_datetime, analysis_neighborhood
        FROM ${CONFIG.motherduck.table}
        WHERE incident_datetime >= NOW() - INTERVAL '24 hours'
        ORDER BY incident_datetime DESC
        LIMIT 5
      `,
      description: 'Show most recent crime incidents'
    },
    {
      name: 'Incidents by Category',
      sql: `
        SELECT incident_category, COUNT(*) as incident_count
        FROM ${CONFIG.motherduck.table}
        GROUP BY incident_category
        ORDER BY incident_count DESC
      `,
      description: 'Crime distribution by category'
    },
    {
      name: 'Incidents by Neighborhood',
      sql: `
        SELECT analysis_neighborhood, COUNT(*) as incident_count
        FROM ${CONFIG.motherduck.table}
        WHERE analysis_neighborhood IS NOT NULL
        GROUP BY analysis_neighborhood
        ORDER BY incident_count DESC
      `,
      description: 'Crime distribution by neighborhood'
    },
    {
      name: 'Incidents by Time of Day',
      sql: `
        SELECT
          hour_of_day,
          COUNT(*) as incident_count,
          ROUND(AVG(latitude), 4) as avg_latitude,
          ROUND(AVG(longitude), 4) as avg_longitude
        FROM ${CONFIG.motherduck.table}
        WHERE hour_of_day IS NOT NULL
        GROUP BY hour_of_day
        ORDER BY hour_of_day
      `,
      description: 'Crime patterns by hour of day with average location'
    },
    {
      name: 'Coordinate Range Validation',
      sql: `
        SELECT
          MIN(latitude) as min_lat,
          MAX(latitude) as max_lat,
          MIN(longitude) as min_lng,
          MAX(longitude) as max_lng,
          COUNT(*) as records_with_coords
        FROM ${CONFIG.motherduck.table}
        WHERE latitude IS NOT NULL AND longitude IS NOT NULL
      `,
      description: 'Validate geographic bounds of data'
    },
    {
      name: 'Data Quality Summary',
      sql: `
        SELECT
          COUNT(*) as total_records,
          COUNT(incident_datetime) as records_with_datetime,
          COUNT(latitude) as records_with_coords,
          COUNT(analysis_neighborhood) as records_with_neighborhood,
          COUNT(DISTINCT incident_category) as unique_categories,
          COUNT(DISTINCT police_district) as unique_districts
        FROM ${CONFIG.motherduck.table}
      `,
      description: 'Overall data quality metrics'
    }
  ];

  for (const query of queries) {
    console.log(`\nğŸ§ª Running Query: ${query.name}`);
    console.log(`ğŸ“ Description: ${query.description}`);
    console.log(`ğŸ” SQL: ${query.sql.trim().replace(/\s+/g, ' ')}`);

    try {
      await new Promise((resolve, reject) => {
        db.all(query.sql, (err, results) => {
          if (err) {
            console.error(`âŒ Query failed: ${err.message}`);
            reject(err);
          } else {
            console.log(`âœ… Query successful: ${results.length} rows returned`);

            // Display results in a formatted way
            if (results.length > 0) {
              console.log('ğŸ“Š Results:');
              results.forEach((row, index) => {
                console.log(`   ${index + 1}. ${JSON.stringify(row, null, 2).replace(/\n/g, ' ')}`);
              });
            } else {
              console.log('ğŸ“‹ No results returned');
            }

            resolve(results);
          }
        });
      });
    } catch (error) {
      console.error(`ğŸ’¥ Failed to execute query "${query.name}":`, error.message);
    }

    // Small delay between queries
    await new Promise(resolve => setTimeout(resolve, 500));
  }
}

/**
 * Main pipeline execution
 */
async function runDataPipelineTest() {
  const startTime = Date.now();

  console.log('ğŸš€ SF Crime Data Pipeline Test: DataSF API â†’ MotherDuck');
  console.log('========================================================');
  console.log(`ğŸ“… Started at: ${new Date().toISOString()}`);
  console.log(`ğŸ¯ Target: ${CONFIG.datasf.recordLimit} records â†’ ${CONFIG.motherduck.database}.${CONFIG.motherduck.table}\n`);

  try {
    // Execute pipeline steps
    const rawData = await fetchCrimeData();
    const transformedData = transformCrimeData(rawData);
    const db = await setupMotherDuckDatabase();
    await createTableAndLoadData(db, transformedData);
    await runAnalyticalQueries(db);

    // Final summary
    const endTime = Date.now();
    const totalTime = endTime - startTime;

    console.log('\nğŸ‰ Data Pipeline Test Completed Successfully!');
    console.log('============================================');
    console.log(`âœ… ${transformedData.length} records processed from DataSF API`);
    console.log(`âœ… Database "${CONFIG.motherduck.database}" created in MotherDuck`);
    console.log(`âœ… Table "${CONFIG.motherduck.table}" created and populated`);
    console.log(`âœ… ${queries.length} analytical queries executed`);
    console.log(`â±ï¸ Total processing time: ${totalTime}ms (${(totalTime/1000).toFixed(2)} seconds)`);

    console.log('\nğŸ’¡ Pipeline Capabilities Verified:');
    console.log('   ğŸ”— DataSF API connectivity and data extraction');
    console.log('   ğŸ”§ Data transformation and cleaning');
    console.log('   ğŸ—„ï¸ MotherDuck database and table creation');
    console.log('   ğŸ“¥ Bulk data loading with proper schema');
    console.log('   ğŸ“Š Complex analytical queries on loaded data');
    console.log('   âš¡ End-to-end processing performance');

    console.log('\nğŸ”„ Next Steps:');
    console.log('   â€¢ Scale up to larger datasets (100s, 1000s of records)');
    console.log('   â€¢ Implement incremental data loading');
    console.log('   â€¢ Add data deduplication logic');
    console.log('   â€¢ Create views for common analytical patterns');
    console.log('   â€¢ Set up automated pipeline scheduling');

  } catch (error) {
    console.error('\nğŸ’¥ Pipeline Test Failed:');
    console.error('Error Type:', error.constructor.name);
    console.error('Message:', error.message);
    if (error.stack) {
      console.error('Stack trace:', error.stack);
    }
    process.exit(1);
  }
}

// Execute the pipeline test
if (require.main === module) {
  runDataPipelineTest()
    .then(() => {
      console.log('\nâœ… Pipeline test completed successfully');
      process.exit(0);
    })
    .catch((error) => {
      console.error('\nğŸ’¥ Pipeline test failed:', error);
      process.exit(1);
    });
}

module.exports = { runDataPipelineTest };
